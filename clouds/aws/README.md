# References

## AWS Inference - Blog links
- [Blog - SageMaker Inference Highspot page](https://aws.highspot.com/items/671814eaf8e8a0fb2ee30a20?lfrm=shp.0)
- [Blog - Scale down down to zero](https://aws.amazon.com/blogs/machine-learning/unlock-costsavings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)
- [Blog - Fast Model loader:](https://aws.amazon.com/blogs/machine-learning/introducing-fastmodel-loader-in-sagemaker-inference-accelerate-autoscaling-for-your-large-languagemodels-llms-part-1/)
- [Blog - Container Caching:](https://aws.amazon.com/blogs/machine-learning/supercharge-your-autoscaling-for-generative-ai-inference-introducing-container-caching-in-sagemaker-inference/)
- [Blog - Updated inference optimization toolkit](https://aws.amazon.com/blogs/machinelearning/:amazon-sagemaker-launches-the-updated-inference-optimization-toolkit-forgenerative-ai/)
- [Blog - Deploy hundreds of LoRA adpaters](https://aws.amazon.com/blogs/machine-learning/easilydeploy-and-manage-hundreds-of-lora-adapters-with-sagemaker-efficient-multi-adapterinference/)
- [Blog - Use Bedrock tooling with SageMaker Jumpstart](https://aws.amazon.com/blogs/machinelearning//use-amazon-bedrock-tooling-with-amazon-sagemaker-jumpstart-models/)
- [Blog - G6e support for SageMaker inference](https://aws.amazon.com/blogs/machinelearning/amazon-sagemaker-inference-now-supports-g6e-instances/)