# References

- [ACL Anthology - Tutorial Proposal: Hallucination in Large Language Models](https://aclanthology.org/2024.lrec-tutorials.11.pdf)
  - [LREC-COLING 2024 - Website for Hallucination in Large Language Models](https://vr25.github.io/lrec-coling-hallucination-tutorial/)
  - [Tutorial Proposal: Hallucination in Large Language Models - Slides](https://vr25.github.io/lrec-coling-hallucination-tutorial/slides/25%20Hallucinations%20in%20Large%20Language%20Models.pdf)
  - [Github - LREC Coling hallunication tutorial](https://github.com/vr25/lrec-coling-hallucination-tutorial)
- [Arxiv - Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis](https://arxiv.org/abs/2411.19655)
- [Arxiv - Lee, Nayeon, et al. Factuality enhanced language models for open-ended text generation.](https://arxiv.org/abs/2206.04624)
- [ACLAnthology - Ladhak, Faisal, et al. When do pre-training biases propagate to downstream tasks? a case study in text summarization.](https://aclanthology.org/2023.eacl-main.234.pdf)
- [The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations.](https://aclanthology.org/2023.emnlp-main.155.pdf) 
  - In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2541â€“2573, Singapore. Association for Computational Linguistics.
- [Github - HaluEval](https://github.com/RUCAIBox/HaluEval)

## Python modules

- [Github - LettuceDetect](https://github.com/KRLabsOrg/LettuceDetect)
  - Key Features:
    - Token-level precision: detect exact hallucinated spans
    - Optimized for inference: smaller model size and faster inference
    - 4K context window via ModernBERT
    - MIT-licensed models & code
    - HF Integration: one-line model loading
    - Easy to use python API: can be downloaded from pip and few lines of code to integrate into your RAG system
- 