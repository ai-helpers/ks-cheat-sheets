{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c321e7-936a-4d9d-9ee6-61b99d020a22",
   "metadata": {},
   "source": [
    "# LLM Example\n",
    "    Source: DeepLearning.AI How Transformer LLMs work\n",
    "\n",
    "In this lesson, you will reinforce your understanding of the transformer architecture by exploring the decoder-only [model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) `microsoft/Phi-3-mini-4k-instruct`.\n",
    "\n",
    "![architecture](_static/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86247fc7-bbd8-4c77-9a4c-6539553b88a0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "401debc8-195d-4586-9a8c-388583b6f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "676e65cf-2f3c-4f89-bbd6-15e9f08587e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.48.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/CORENTIN/.pyenv/versions/3.10.14/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "---\n",
      "Name: torch\n",
      "Version: 2.2.2\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /Users/CORENTIN/.pyenv/versions/3.10.14/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate\n",
      "---\n",
      "Name: accelerate\n",
      "Version: 1.3.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /Users/CORENTIN/.pyenv/versions/3.10.14/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show transformers torch accelerate #numpy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093d7ea-0744-4947-982e-4b4e7ae85325",
   "metadata": {},
   "source": [
    "Let's first load the model and its tokenizer. For that you will first import the classes: `AutoModelForCausalLM` and `AutoTokenizer`. When you want to process a sentence, you can apply the tokenizer first and then the model in two separate steps. Or you can create a pipeline object that wraps the two steps and then apply the pipeline to the sentence. You'll explore both approaches in this notebook. This is why you'll also import the `pipeline` class.\n",
    "\n",
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>FYI: </b> The transformers library has two types of model classes: <code> AutoModelForCausalLM </code> and <code>AutoModelForMaskedLM</code>. Causal language models represent the decoder-only models that are used for text generation. They are described as causal, because to predict the next token, the model can only attend to the preceding left tokens. Masked language models represent the encoder-only models that are used for rich text representation. They are described as masked, because they are trained to predict a masked or hidden token in a sequence.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1b626cf-c23f-4a07-9606-3a2327017511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281f490dc4cb49e99268c8c5920d1896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./models/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False, # False means to not include the prompt text in the returned text\n",
    "    max_new_tokens=50, \n",
    "    do_sample=False, # no randomness in the generated text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b805f-2acb-4a1d-9d70-eec5f37a262b",
   "metadata": {},
   "source": [
    "You'll now use the pipeline object (labeled as generator) to generate a response consisting of 50 tokens to the given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12d54673-6449-4f18-9063-22e5122df113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email to Sarah for the tragic gardening mishap.\"\n",
    "output = generator(prompt)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b71f68-826a-4d1c-ab68-9d5d12e3d6e3",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eb950bf-f4ac-4011-98ad-131e1602c157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0ec7c8c-39d6-4125-b426-07676b376f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32064, 3072, padding_idx=32000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff5c5cf-8612-4ef3-ba38-f6bfc4bb66c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3Model(\n",
       "  (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x Phi3DecoderLayer(\n",
       "      (self_attn): Phi3Attention(\n",
       "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Phi3MLP(\n",
       "        (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (activation_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): Phi3RMSNorm()\n",
       "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (post_attention_layernorm): Phi3RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e389d987-fb4e-4979-8936-e8f34061a1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3DecoderLayer(\n",
       "  (self_attn): Phi3Attention(\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (mlp): Phi3MLP(\n",
       "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (activation_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): Phi3RMSNorm()\n",
       "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_attention_layernorm): Phi3RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 32 transformer blocks or layers. You can access any particular block.\n",
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adef48c-da8a-45f5-afe8-227451c17201",
   "metadata": {},
   "source": [
    "## Generating a Single Token to a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498c792-8d5d-49a5-b8fd-990194a57e6d",
   "metadata": {},
   "source": [
    "You earlier used the Pipeline object to generate a text response to a prompt. The pipeline provides an abstraction to the underlying process of text generation. Each token in the text is actually generated one by one.\n",
    "\n",
    "Let's now give the model a prompt and check the first token it will generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96171274-8a5d-44e1-ae7f-b993c0ed9243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 450, 7483,  310, 3444,  338]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "# You'll need first to tokenize the prompt and get the ids of the tokens.\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc7d88-687f-4440-91f7-6561062652b0",
   "metadata": {},
   "source": [
    "Let's now pass the token ids to the transformer block (before the LM head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb5b2568-fa9f-4140-b818-0e4ea6cac37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the output of the model before the lm_head\n",
    "model_output = model.model(input_ids)\n",
    "\n",
    "# Get the shape the output the model before the lm_head\n",
    "model_output[0].shape \n",
    "# > The transformer block outputs for each token a vector of size 3072 (embedding size). Let's check the shape of this output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea64946-e320-4082-8a19-e262a07055d6",
   "metadata": {},
   "source": [
    "The first number represents the batch size, which is 1 in this case since we have one prompt. The second number 5 represents the number of tokens. And finally 3072 represents the embedding size (the size of the vector that corresponds to each token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15d7dfff-2005-422f-956a-68e9dab6fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now get the output of the LM head.\n",
    "lm_head_output = model.lm_head(model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "024a0acc-d292-483c-9c47-de0008c586ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32064])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c45dbc-0102-4083-b741-567df96e956e",
   "metadata": {},
   "source": [
    "The LM head outputs for each token in the input prompt, a vector of size 32064 (vocabulary size). So there are 5 vectors, each of size 32064. Each vector can be mapped to a probability distribution, that shows the probability for each token in the vocabulary to come after the given token in the input prompt.\n",
    "\n",
    "Since we're interested in generating the output token that comes after the last token in the input prompt (\"is\"), we'll focus on the last vector. So in the next cell, lm_head_output[0,-1] is a vector of size 32064 from which you can generate the token that comes after (\"is\"). You can do that by finding the id of the token that corresponds to the highest value in the vector lm_head_output[0,-1] (using argmax(-1), -1 means across the last axis here).a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec396cd2-7e8b-46ef-b4ef-773d90f2f43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3681)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = lm_head_output[0,-1].argmax(-1)\n",
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cad9a6e8-3715-4c64-9756-c3f8c04a2d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, let's decode the returned token id.\n",
    "tokenizer.decode(token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
